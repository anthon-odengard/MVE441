{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/Fish3.txt', delimiter=' ')\n",
    "feature_names = ['Weight', 'L1', 'L2', 'L3', 'Height', 'Width']\n",
    "print(data_df)\n",
    "# Split labels and features.\n",
    "labels_df = data_df[\"Species\"]\n",
    "features_df = data_df[feature_names].abs()\n",
    "\n",
    "labels = labels_df.copy().to_numpy()\n",
    "features = features_df.copy().to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_train, bins=7, edgecolor='k', alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "X_train_scaled = standard_scaler.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =PCA(n_components=6)\n",
    "principel_components = pca.fit_transform(X_train_scaled)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(principel_components, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"])\n",
    "df_pca_labels = pd.DataFrame(y_train, columns=[\"label\"])\n",
    "df_pca_and_label = pd.concat([df_pca, df_pca_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_component_1D(df_final, pc_index):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    targets = df_final['label'].unique()\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y',] \n",
    "    for target, color in zip(targets, colors):\n",
    "        indices_to_keep = df_final['label'] == target\n",
    "        plt.scatter(df_final.loc[indices_to_keep, f'PC{pc_index+1}'],\n",
    "                    np.zeros_like(df_final.loc[indices_to_keep, f'PC{pc_index+1}']), \n",
    "                    c=color,\n",
    "                    label=target,\n",
    "                    alpha=0.3)\n",
    "    plt.xlabel(f'Principal Component {pc_index}', fontsize=14)\n",
    "    plt.title(f'PCA of Dataset (PC{pc_index})', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc_index in range(0,6):\n",
    "    plot_principal_component_1D(df_pca_and_label, pc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components_2D(df_final, pc1_index, pc2_index):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    targets = df_final['label'].unique()\n",
    "    colors = colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange']\n",
    "  # Define colors for different classes\n",
    "    for target, color in zip(targets, colors):\n",
    "        indices_to_keep = df_final['label'] == target\n",
    "        plt.scatter(df_final.loc[indices_to_keep, f'PC{pc1_index+1}'],\n",
    "                    df_final.loc[indices_to_keep, f'PC{pc2_index+1}'],\n",
    "                    c=color,\n",
    "                    label=target,\n",
    "                    alpha=0.3)\n",
    "    plt.xlabel(f'Principal Component {pc1_index+1}', fontsize=14)\n",
    "    plt.ylabel(f'Principal Component {pc2_index+1}', fontsize=14)\n",
    "    plt.title(f'PCA of Dataset (PC{pc1_index+1} vs PC{pc2_index+1})', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components_3D(df_final, pc1_index, pc2_index, pc3_index):\n",
    "    \"\"\"\n",
    "    Plot 3D scatter plot of principal components.\n",
    "\n",
    "    Parameters:\n",
    "    df_final (DataFrame): DataFrame containing principal components and labels.\n",
    "    pc1_index (int): Index of the first principal component.\n",
    "    pc2_index (int): Index of the second principal component.\n",
    "    pc3_index (int): Index of the third principal component.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    targets = df_final['label'].unique()\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange']\n",
    "    for target, color in zip(targets, colors):\n",
    "        indices_to_keep = df_final['label'] == target\n",
    "        ax.scatter(df_final.loc[indices_to_keep, f'PC{pc1_index+1}'],\n",
    "                   df_final.loc[indices_to_keep, f'PC{pc2_index+1}'],\n",
    "                   df_final.loc[indices_to_keep, f'PC{pc3_index+1}'],\n",
    "                   c=color,\n",
    "                   label=target,\n",
    "                   alpha=0.3)\n",
    "    ax.set_xlabel(f'Principal Component {pc1_index+1}', fontsize=14)\n",
    "    ax.set_ylabel(f'Principal Component {pc2_index+1}', fontsize=14)\n",
    "    ax.set_zlabel(f'Principal Component {pc3_index+1}', fontsize=14)\n",
    "    ax.set_title(f'PCA of Dataset (PC{pc1_index+1} vs PC{pc2_index+1} vs PC{pc3_index+1})', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = list(combinations(range(3), 2))\n",
    "\n",
    "for pairs in all_combinations:\n",
    "    plot_principal_components_2D(df_pca_and_label,pairs[0],pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to plot principal components in 2D\n",
    "def plot_principal_components_2D(df_final, pc1_index, pc2_index, ax):\n",
    "    targets = df_final['label'].unique()\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange']  # Define colors for different classes\n",
    "    for target, color in zip(targets, colors):\n",
    "        indices_to_keep = df_final['label'] == target\n",
    "        ax.scatter(df_final.loc[indices_to_keep, f'PC{pc1_index+1}'],\n",
    "                   df_final.loc[indices_to_keep, f'PC{pc2_index+1}'],\n",
    "                   c=color,\n",
    "                   label=target,\n",
    "                   alpha=0.3)\n",
    "    ax.set_xlabel(f'Principal Component {pc1_index+1}', fontsize=14)\n",
    "    ax.set_ylabel(f'Principal Component {pc2_index+1}', fontsize=14)\n",
    "    ax.set_title(f'PCA of Dataset (PC{pc1_index+1} vs PC{pc2_index+1})', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Create a 1x3 subplot arrangement\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "# Get all combinations of pairs of principal components\n",
    "all_combinations = list(combinations(range(3), 2))\n",
    "\n",
    "# Plot each pair of principal components in a subplot\n",
    "for i, pairs in enumerate(all_combinations):\n",
    "    plot_principal_components_2D(df_pca_and_label, pairs[0], pairs[1], ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes not well separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote = StandardScaler().fit_transform(X_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train, bins=7, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Original data distribution\")\n",
    "plt.grid(False)  # Turn off the grid\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_train_smote, bins=7, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Upsampled data distribution\")\n",
    "plt.grid(False)  # Turn off the grid\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_smote = PCA(n_components=6)\n",
    "principel_components_smote = PCA_smote.fit_transform(X_train_smote)\n",
    "print(PCA_smote.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_smote = pd.DataFrame(principel_components_smote, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\"])\n",
    "df_pca_labels_smote = pd.DataFrame(y_train_smote, columns=[\"label\"])\n",
    "df_pca_and_label_smote = pd.concat([df_pca_smote, df_pca_labels_smote], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "# Get all combinations of pairs of principal components\n",
    "all_combinations = list(combinations(range(3), 2))\n",
    "\n",
    "# Plot each pair of principal components in a subplot\n",
    "for i, pairs in enumerate(all_combinations):\n",
    "    plot_principal_components_2D(df_pca_and_label_smote, pairs[0], pairs[1], ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_scores_cv(scores):\n",
    "\n",
    "    table = [[\"Sample\", \"Accuracy\", \"F1-score\"]]\n",
    "    for i, (score1, score2) in enumerate(scores, start=1):\n",
    "        table.append([i, f\"{score1:.2f}\", f\"{score2:.2f}\"])\n",
    "\n",
    "    print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "    \n",
    "def print_scores(scores):\n",
    "    table = [[\"Iteration\", \"Overall Accuracy\", \"Overall Recall\", \"Overall Specificity\"]]\n",
    "    for i, metrics in enumerate(scores, start=1):\n",
    "        overall_accuracy = metrics['overall_accuracy']\n",
    "        overall_recall = metrics['overall_recall']\n",
    "        overall_specificity = metrics['overall_specificity']\n",
    "        table.append([i, f\"{overall_accuracy:.2f}\", f\"{overall_recall:.2f}\", f\"{overall_specificity:.2f}\"])\n",
    "    print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))\n",
    "    \n",
    "def print_scores_class(evaluation_metrics):\n",
    "    # Initialize dictionaries to store cumulative sums of metrics for each class\n",
    "    class_metrics_sum = {label: {'accuracy': 0, 'recall': 0, 'specificity': 0} for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    class_counts = {label: 0 for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    \n",
    "    # Calculate cumulative sums of metrics for each class\n",
    "    for metrics in evaluation_metrics:\n",
    "        for label, class_metrics in metrics['class_specific_metrics'].items():\n",
    "            class_metrics_sum[label]['accuracy'] += class_metrics['accuracy']\n",
    "            class_metrics_sum[label]['recall'] += class_metrics['recall']\n",
    "            class_metrics_sum[label]['specificity'] += class_metrics['specificity']\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    # Calculate average metrics for each class\n",
    "    class_metrics_avg = {label: {metric: class_metrics_sum[label][metric] / class_counts[label] for metric in class_metrics_sum[label]} for label in class_metrics_sum}\n",
    "    \n",
    "    # Create a table with class labels as columns and metric averages as rows\n",
    "    table = [[\"Class\"] + list(class_metrics_avg.keys())]\n",
    "    table.append([\"Accuracy\"] + [f\"{class_metrics_avg[label]['accuracy']:.2f}\" for label in class_metrics_avg])\n",
    "    table.append([\"Recall\"] + [f\"{class_metrics_avg[label]['recall']:.2f}\" for label in class_metrics_avg])\n",
    "    table.append([\"specificity\"] + [f\"{class_metrics_avg[label]['specificity']:.2f}\" for label in class_metrics_avg])\n",
    "    \n",
    "    # Print the table\n",
    "    print(tabulate(table, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "X_train_upsampled, y_train_upsampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "scaler = StandardScaler()\n",
    "X_train_upsampled = scaler.fit_transform(X_train_upsampled)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_evaluation(model, param_grid, features, labels):\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1, refit='f1_macro', scoring=['accuracy', 'f1_macro'])\n",
    "    grid_search.fit(features, labels)\n",
    "\n",
    "    # Get the best parameters and evaluate the model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    # Evaluate the model\n",
    "    print(\"Best Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_class_specific_performance(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    metrics= {}\n",
    "   \n",
    "    for index, label in enumerate(labels):\n",
    "        TP = cm[index, index]\n",
    "        FP = cm[:,index].sum() - TP\n",
    "        FN = cm[:, index].sum() - TP\n",
    "        TN = cm.sum() - TP - FP - FN\n",
    "        \n",
    "        accuracy = (TP + TN) /(TP+ FP+ FN+ TN)\n",
    "        recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        \n",
    "        metrics[label] = {'accuracy': accuracy, \n",
    "                          'recall': recall,\n",
    "                          'specificity': specificity}\n",
    "    return metrics\n",
    "\n",
    "def specificity_score(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    metrics= {}\n",
    "    for index, label in enumerate(labels):\n",
    "        TP = cm[index, index]\n",
    "        FP = cm[:,index].sum() - TP\n",
    "        FN = cm[:, index].sum() - TP\n",
    "        TN = cm.sum() - TP - FP - FN\n",
    "        \n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        \n",
    "    specificity = np.mean(specificity)\n",
    "    \n",
    "    return specificity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, features, labels, iterations):\n",
    "    evaluation_metrics = []\n",
    "    evaluation_metrics_class = []\n",
    "    smote = SMOTE(random_state=42)\n",
    "    unique_labels= np.unique(labels)\n",
    "    for iteration in range(0, iterations):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "        X_train_upsampled, y_train_upsampled = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_upsampled = scaler.fit_transform(X_train_upsampled)\n",
    "        \n",
    "        model.fit(X_train_upsampled, y_train_upsampled)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        recall = recall_score(y_test, predictions, average='macro')\n",
    "        specificity = specificity_score(y_test, predictions, unique_labels)\n",
    "        class_specific_metrics = eval_class_specific_performance(y_test, predictions, unique_labels)\n",
    "        evaluation_metrics.append({\n",
    "            'overall_accuracy': accuracy,\n",
    "            'overall_recall': recall,\n",
    "            'overall_specificity': specificity\n",
    "        })\n",
    "        \n",
    "        evaluation_metrics_class.append({'class_specific_metrics': class_specific_metrics})\n",
    "        \n",
    "    return evaluation_metrics, evaluation_metrics_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1 ,1 ,2, 5, 10,15,20]\n",
    "}\n",
    "\n",
    "log_regression_model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "grid_evaluation(log_regression_model, param_grid, X_train_upsampled,y_train_upsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neigbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,2,5,10,15,20]\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsClassifier()\n",
    "grid_evaluation(knn_model, param_grid, X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_evaluation(random_forest_classifier, param_grid, X_train_upsampled, y_train_upsampled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train_upsampled)\n",
    "y_test_encoded = encoder.transform(y_test)\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7, 10, 20],\n",
    "    'learning_rate': [0.01, 0.1, 0.3, 1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "grid_evaluation(xgb_classifier, param_grid_xgb, X_train_upsampled, y_train_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen']  # Only applicable if solver is 'lsqr' or 'eigen'\n",
    "}\n",
    "\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "\n",
    "grid_evaluation(lda_model, param_grid, X_train_upsampled, y_train_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(-12, 0, 50)\n",
    "}\n",
    "\n",
    "gaussian_NB_model = GaussianNB()\n",
    "grid_evaluation(gaussian_NB_model, param_grid, X_train_upsampled, y_train_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression(C=15, penalty='l1', solver='liblinear')\n",
    "logistic_regression_perfomance, logistic_regression_perfomance_class = evaluate_model_performance(log_regression_model, features, labels, 100)\n",
    "print(logistic_regression_perfomance)\n",
    "print_scores(logistic_regression_perfomance)\n",
    "print_scores_class(logistic_regression_perfomance_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_model_performance, knn_model_performance_class = evaluate_model_performance(knn_model, features, labels, 100)\n",
    "#print_scores(knn_model_performance)\n",
    "print_scores_class(knn_model_performance_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(max_depth= 20, min_samples_leaf=1, min_samples_split=2, n_estimators= 100)\n",
    "random_forest_performace, random_forest_performace_class = evaluate_model_performance(knn_model, features, labels, 100)\n",
    "print_scores(random_forest_performace)\n",
    "print(random_forest_performace_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder().fit(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(colsample_bytree = 0.8, learning_rate= 0.1, max_depth=20, n_estimators= 200, subsample= 0.8)\n",
    "xgb_performance, xgb_performance_class = evaluate_model_performance(xgb_model, features, encoder.transform(labels), 100)\n",
    "print_scores(xgb_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda_performance, lda_performance_class = evaluate_model_performance(lda_model, features, encoder.transform(labels), 100)\n",
    "print_scores(lda_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_model =GaussianNB(var_smoothing=1e-12)\n",
    "gaussian_performance, gaussian_performance_class = evaluate_model_performance(gaussian_model, features, labels, 100)\n",
    "print_scores(gaussian_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paired test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfomance_data = [logistic_regression_perfomance, knn_model_performance, random_forest_performace, xgb_performance, lda_performance, gaussian_performance]\n",
    "models = ['LogisticRegression', 'KNeighbors', 'RandomForest', 'XGBoost', 'LDA', 'NaiveBayes']\n",
    "# Function to create a DataFrame for a specific metric\n",
    "def create_metric_df(all_data, metric_index, metric_name, models):\n",
    "    df_list = []\n",
    "    for i, data in enumerate(all_data):\n",
    "        model_name = models[i]  # Get the model name from the models list\n",
    "        for metric_value in data:\n",
    "          \n",
    "            df_list.append({'Model': model_name, metric_name: metric_value[metric_index]})\n",
    "    return pd.DataFrame(df_list)\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "accuracy_df = create_metric_df(perfomance_data, 'overall_accuracy', 'Accuracy', models)\n",
    "specificity_df = create_metric_df(perfomance_data, 'overall_specificity', 'Specificity', models)\n",
    "recall_df = create_metric_df(perfomance_data, 'overall_recall', 'Recall', models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create boxplot for accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Model', y='Accuracy', data=accuracy_df)\n",
    "plt.title('Boxplot of Accuracy Across Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Create boxplot for precision\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Model', y='Specificity', data=specificity_df)\n",
    "plt.title('Boxplot of Specificity Across Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Create boxplot for recall\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Model', y='Recall', data=recall_df)\n",
    "plt.title('Boxplot of Recall Across Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Create boxplot for accuracy\n",
    "sns.boxplot(ax=axes[0], x='Model', y='Accuracy', data=accuracy_df)\n",
    "axes[0].set_title('Boxplot of Accuracy Across Models')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Create boxplot for specificity\n",
    "sns.boxplot(ax=axes[1], x='Model', y='Specificity', data=specificity_df)\n",
    "axes[1].set_title('Boxplot of Specificity Across Models')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Create boxplot for recall\n",
    "sns.boxplot(ax=axes[2], x='Model', y='Recall', data=recall_df)\n",
    "axes[2].set_title('Boxplot of Recall Across Models')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metric_average(evaluation_metrics):\n",
    "    # Initialize dictionaries to store cumulative sums of metrics for each class\n",
    "    class_metrics_sum = {label: {'accuracy': 0, 'recall': 0, 'specificity': 0} for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    class_counts = {label: 0 for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    \n",
    "    # Calculate cumulative sums of metrics for each class\n",
    "    for metrics in evaluation_metrics:\n",
    "        for label, class_metrics in metrics['class_specific_metrics'].items():\n",
    "            class_metrics_sum[label]['accuracy'] += class_metrics['accuracy']\n",
    "            class_metrics_sum[label]['recall'] += class_metrics['recall']\n",
    "            class_metrics_sum[label]['specificity'] += class_metrics['specificity']\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    # Calculate average metrics for each class\n",
    "    class_metrics_avg = {label: {metric: class_metrics_sum[label][metric] / class_counts[label] for metric in class_metrics_sum[label]} for label in class_metrics_sum}\n",
    "    return class_metrics_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_performance_data = [logistic_regression_perfomance_class, knn_model_performance_class, random_forest_performace_class,\n",
    "                               xgb_performance_class, lda_performance_class, gaussian_performance_class]\n",
    "\n",
    "class_wise_performance = {}\n",
    "for model, model_class_metrics in zip(models, class_wise_performance_data):\n",
    "    class_wise_performance[model]= class_metric_average(model_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = {}\n",
    "recall_data = {}\n",
    "specificity_data = {}\n",
    "\n",
    "# Aggregate class-wise metrics for each model\n",
    "for model, class_metrics in class_wise_performance.items():\n",
    "    accuracy_data[model] = [metrics['accuracy'] for metrics in class_metrics.values()]\n",
    "    recall_data[model] = [metrics['recall'] for metrics in class_metrics.values()]\n",
    "    specificity_data[model] = [metrics['specificity'] for metrics in class_metrics.values()]\n",
    "\n",
    "# Convert aggregated metrics into a DataFrame\n",
    "accuracy_class_wise_df = pd.DataFrame(accuracy_data, index=class_metrics.keys())\n",
    "recall_class_wise_df = pd.DataFrame(recall_data, index=class_metrics.keys())\n",
    "specificity_class_wise_df = pd.DataFrame(specificity_data, index=class_metrics.keys())\n",
    "\n",
    "# Plot the aggregated metric-wise bar plots for each model\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "\n",
    "accuracy_class_wise_df.plot(kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_title('Accuracy by Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "recall_class_wise_df.plot(kind='bar', ax=axes[1], rot=45)\n",
    "axes[1].set_title('Recall by Model')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "specificity_class_wise_df.plot(kind='bar', ax=axes[2], rot=45)\n",
    "axes[2].set_title('Specificity by Model')\n",
    "axes[2].set_ylabel('Specificity')\n",
    "axes[2].legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# For example:\n",
    "print(\"This print statement will not be suppressed.\")\n",
    "# Function to perform paired Wilcoxon signed-rank test for the same metric across models\n",
    "def perform_paired_test(df, metric_name):\n",
    "    model_names = df['Model'].unique()\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(i+1, len(model_names)):\n",
    "            model1 = model_names[i]\n",
    "            model2 = model_names[j]\n",
    "            data1 = df[df['Model'] == model1][metric_name]\n",
    "            data2 = df[df['Model'] == model2][metric_name]\n",
    "            stat, p = wilcoxon(data1, data2)\n",
    "            print(f\"Models: {model1} vs {model2}, Metric: {metric_name}\")\n",
    "            print(f\"Statistic: {stat}, p-value: {p}\\n\")\n",
    "\n",
    "# Perform paired tests for accuracy\n",
    "perform_paired_test(accuracy_df, 'Accuracy')\n",
    "\n",
    "# Perform paired tests for precision\n",
    "perform_paired_test(specificity_df, 'Specificity')\n",
    "\n",
    "# Perform paired tests for recall\n",
    "perform_paired_test(recall_df, 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(model):\n",
    "    return model.__class__.__name__\n",
    "\n",
    "\n",
    "def feature_importance(model, iterations, features, labels):\n",
    "    feature_importances = []\n",
    "    feature_importance_missclass = []\n",
    "    model_missclass = clone(model)\n",
    "\n",
    "    for iteration in tqdm(range(0, iterations)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "        X_train_upsampled, y_train_upsampled = smote.fit_resample(X_train, y_train)\n",
    "        model_name = get_model_name(model)\n",
    "        if model_name == 'XGBClassifier':\n",
    "            encoder = LabelEncoder()\n",
    "            y_train_upsampled = encoder.fit_transform(y_train_upsampled)\n",
    "            y_test = encoder.transform(y_test)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_upsampled = scaler.fit_transform(X_train_upsampled)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train_upsampled, y_train_upsampled)\n",
    "        \n",
    "        missclassified_istances = np.where(model.predict(X_test) != y_test)[0]\n",
    "        model_missclass.fit(X_train_upsampled[missclassified_istances], y_train_upsampled[missclassified_istances])\n",
    "        model_name = get_model_name(model)\n",
    "        \n",
    "        if model_name == 'LogisticRegression':\n",
    "            importance = model.coef_[0]\n",
    "            importance_missclass = model_missclass.coef_[0]\n",
    "            feature_importances.append({'Model':model_name , 'Feature importance': importance})\n",
    "            feature_importance_missclass.append({'Model':model_name , 'Feature importance': importance_missclass})\n",
    "        elif model_name == 'RandomForestClassifier':\n",
    "            importance = model.feature_importances_\n",
    "            importance_missclass = model_missclass.feature_importances_\n",
    "            feature_importances.append({'Model':model_name , 'Feature importance': importance})\n",
    "            feature_importance_missclass.append({'Model':model_name , 'Feature importance': importance_missclass})\n",
    "        else:\n",
    "            perm_importances = permutation_importance(model, X_test, y_test, n_repeats=10)\n",
    "            perm_importance_missclass = permutation_importance(model_missclass, X_test, y_test, n_repeats=10)\n",
    "            feature_importances.append({'Model':model_name , 'Feature importance': perm_importances.importances_mean})\n",
    "            feature_importance_missclass.append({'Model':model_name , 'Feature importance': perm_importance_missclass.importances_mean})\n",
    "            \n",
    "        \n",
    "    return feature_importances, feature_importance_missclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_importances = []\n",
    "all_feature_importances_miss = []\n",
    "log_reg_feature_importance, log_reg_feature_importance_miss = feature_importance(logistic_regression_model,10, features, labels)\n",
    "all_feature_importances.append(log_reg_feature_importance)\n",
    "all_feature_importances_miss.append(log_reg_feature_importance_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_feature_importance, knn_feature_importance_miss = feature_importance(knn_model, 10, features, labels)\n",
    "all_feature_importances.append(knn_feature_importance)\n",
    "all_feature_importances_miss.append(knn_feature_importance_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_importance, random_forest_importance_miss = feature_importance(random_forest_model, 10, features, labels)\n",
    "all_feature_importances.append(random_forest_importance)\n",
    "all_feature_importances_miss.append(random_forest_importance_miss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_feature_importance, gaussian_feature_importance_miss = feature_importance(gaussian_model,10, features, labels)\n",
    "all_feature_importances.append(gaussian_feature_importance)\n",
    "all_feature_importances_miss.append(gaussian_feature_importance_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_feature_importance, lda_feature_importance_miss = feature_importance(lda_model, 10, features, labels)\n",
    "all_feature_importances.append(lda_feature_importance)\n",
    "all_feature_importances_miss.append(lda_feature_importance_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_importance, xgb_feature_importance_miss= feature_importance(xgb_model, 10, features, labels)\n",
    "all_feature_importances.append(xgb_feature_importance)\n",
    "all_feature_importances_miss.append(xgb_feature_importance_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_importance_df(model_data):\n",
    "    df_list = []\n",
    "    for models in model_data:\n",
    "        for model_dict in models:\n",
    "            model_name = model_dict['Model']\n",
    "            feature_importances = model_dict['Feature importance']\n",
    "            model_data = {'Model': model_name}\n",
    "            for i, importance in enumerate(feature_importances):\n",
    "                model_data[feature_names[i]] = importance\n",
    "            df_list.append(model_data)\n",
    "    return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = np.unique(feature_names)\n",
    "feature_importance_df = create_importance_df(all_feature_importances)\n",
    "feature_importance_miss_df = create_importance_df(all_feature_importances_miss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def plot_model_boxplot(data, model_name):\n",
    "    # Filter the DataFrame to get data for the specified model\n",
    "    model_data = data[data['Model'] == model_name]\n",
    "   \n",
    "    # Extract feature names and feature importance values\n",
    "    feature_names = model_data.columns[1:]  # Exclude 'Model' column\n",
    "    feature_values = model_data.iloc[:, 1:].values  # Transpose for boxplot\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    plot_data = pd.DataFrame(feature_values, columns=feature_names)\n",
    "   \n",
    "    \n",
    "    # Plot the boxplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=plot_data)\n",
    "    plt.title(f'Feature Importance Boxplot for Model: {model_name}')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    '''\n",
    "    \n",
    "def plot_model_boxplot(data, model_names):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    for i, model_name in enumerate(model_names):\n",
    "        # Filter the DataFrame to get data for the specified model\n",
    "        model_data = data[data['Model'] == model_name]\n",
    "\n",
    "        # Extract feature names and feature importance values\n",
    "        feature_names = model_data.columns[1:]  # Exclude 'Model' column\n",
    "        feature_values = model_data.iloc[:, 1:].values  # Transpose for boxplot\n",
    "\n",
    "        # Create a DataFrame for plotting\n",
    "        plot_data = pd.DataFrame(feature_values, columns=feature_names)\n",
    "\n",
    "        # Plot the boxplot\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        sns.boxplot(data=plot_data, ax=axes[row, col])\n",
    "        axes[row, col].set_title(f'Feature Importance Boxplot for Model: {model_name}')\n",
    "        axes[row, col].set_xlabel('Features')\n",
    "        axes[row, col].set_ylabel('Feature Importance')\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_importance = np.unique(feature_importance_df['Model'])\n",
    "\n",
    "plot_model_boxplot(feature_importance_df, models_importance)\n",
    "    \n",
    "plot_model_boxplot(feature_importance_miss_df, models_importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_random_features(df, column, num_columns):\n",
    "    for i in range(num_columns):\n",
    "        col_name = f'Random_feature_{i}'\n",
    "        df[col_name] = np.random.normal(loc=0, scale=100, size=len(df))\n",
    "    return df\n",
    "\n",
    "def add_correlated_features(df, column, num_columns):\n",
    "    for i in range(num_columns):\n",
    "        col_index = np.random.randint(low=0, high=df.shape[1])\n",
    "        col_name = f'Correlated_feature_{i}'\n",
    "        df[col_name] = df.iloc[:, col_index] * np.random.randint(1,10)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model_performance_deteroration(model, features, labels, iterations, number_of_columns, column_type):\n",
    "    \n",
    "    evaluate_metrics_deteroration = []\n",
    "    evaluation_metrics_class_deteroration = []\n",
    "    features_df  = pd.DataFrame(features)\n",
    "    model_name = get_model_name(model)\n",
    "    features_added = 0\n",
    "    for column in tqdm(range(0, number_of_columns)):\n",
    "        if column_type == \"random\":\n",
    "            features_added = add_random_features(features_df.copy(), column, column)\n",
    "        elif column_type == \"correlated\":\n",
    "            features_added = add_correlated_features(features_df.copy(), column, column)\n",
    "        model_performance, model_performance_class = evaluate_model_performance(model, features_added.to_numpy(), labels, iterations)\n",
    "        model_performance = pd.DataFrame(model_performance)\n",
    "        model_average_performance = model_performance.mean(axis=0)\n",
    "        evaluate_metrics_deteroration.append(model_average_performance.to_dict())\n",
    "        \n",
    "    return evaluate_metrics_deteroration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_data_random = evaluate_model_performance_deteroration(lda_model, features, labels, 50, 100, \"random\")\n",
    "knn_data_random = evaluate_model_performance_deteroration(knn_model, features, labels, 50, 100, \"random\")\n",
    "\n",
    "lda_data_correlated = evaluate_model_performance_deteroration(lda_model, features, labels, 50, 100, \"correlated\")\n",
    "knn_data_correlated = evaluate_model_performance_deteroration(knn_model, features, labels, 50, 100, \"correlated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_noisy_data(data, models):\n",
    "    # Plotting the data\n",
    "    accuracy = [entry[\"overall_accuracy\"] for entry in data]\n",
    "    recall = [entry[\"overall_recall\"] for entry in data]\n",
    "    specificity = [entry[\"overall_specificity\"] for entry in data]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot( accuracy, label='Overall Accuracy', marker='o')\n",
    "    plt.plot(recall, label='Overall Recall', marker='o')\n",
    "    plt.plot(specificity, label='Overall Specificity', marker='o')\n",
    "\n",
    "    # Adding titles and labels\n",
    "    plt.title('Metrics Over Time')\n",
    "    plt.xlabel('Adda features')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "def plot_noisy_data_2(datasets):\n",
    "    # Create a 2x2 subplot grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    row_titles = ['LDA', 'KNN']\n",
    "    for idx, data in enumerate(datasets):\n",
    "        # Plotting the data\n",
    "        accuracy = [entry[\"overall_accuracy\"] for entry in data]\n",
    "        recall = [entry[\"overall_recall\"] for entry in data]\n",
    "        specificity = [entry[\"overall_specificity\"] for entry in data]\n",
    "\n",
    "        # Plot metrics\n",
    "        axs[idx // 2, idx % 2].plot(accuracy, label='Overall Accuracy')\n",
    "        axs[idx // 2, idx % 2].plot(recall, label='Overall Recall')\n",
    "        axs[idx // 2, idx % 2].plot(specificity, label='Overall Specificity')\n",
    "\n",
    "        # Adding titles and labels\n",
    "        if idx % 2 == 0:\n",
    "            axs[idx // 2, idx % 2].set_title(f'Random noise')\n",
    "        else:\n",
    "            axs[idx // 2, idx % 2].set_title(f'Correlated noise')\n",
    "        \n",
    "        axs[idx // 2, idx % 2].set_xlabel('Added features')\n",
    "        axs[idx // 2, idx % 2].set_ylabel('Metrics')\n",
    "        axs[idx // 2, idx % 2].legend()\n",
    "        axs[idx // 2, idx % 2].grid(True)\n",
    "        \n",
    "        # Set row titles\n",
    "        if idx % 2 == 0:\n",
    "            axs[idx // 2, idx % 2].text(-0.2, 1.2, row_titles[idx // 2], transform=axs[idx // 2, idx % 2].transAxes, \n",
    "                                        fontsize=16, fontweight='bold', ha='center', va='center')\n",
    "\n",
    "    # Adjust layout to prevent overlap and add space between plots\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.6)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_festures_data = [lda_data_random, lda_data_correlated, knn_data_random, knn_data_correlated]\n",
    "print(added_festures_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_noisy_data_2(added_festures_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_features_np(features, num_columns):\n",
    "    for i in range(num_columns):\n",
    "        random_features = np.random.normal(loc=0, scale=100, size=len(features))\n",
    "        features = np.column_stack((features, random_features))\n",
    "    return features\n",
    "\n",
    "def add_correlated_features_np(features, num_columns):\n",
    "    for i in range(num_columns):\n",
    "        col_index = np.random.randint(low=0, high=5)\n",
    "        noise = np.random.normal(loc=0, scale=10, size=len(features))\n",
    "        correlated_features = features[:, col_index] * 0.7 + noise\n",
    "        features = np.column_stack((features, correlated_features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_100_columns = add_random_features_np(features.copy(), 30)\n",
    "random_300_columns = add_random_features_np(features.copy(), 100)\n",
    "\n",
    "# Create datasets with different numbers of correlated columns\n",
    "correlated_100_columns = add_correlated_features_np(features.copy(), 30)\n",
    "correlated_300_columns = add_correlated_features_np(features.copy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_100_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_importance = []\n",
    "\n",
    "lda_feature_importance_100_random = feature_importance(lda_model,1, random_100_columns, labels)\n",
    "lda_feature_importance_300_random = feature_importance(lda_model,1, random_300_columns, labels)\n",
    "\n",
    "knn_feature_importance_100_random = feature_importance(knn_model,1, random_100_columns, labels)\n",
    "knn_feature_importance_300_random = feature_importance(knn_model, 1, random_300_columns, labels)\n",
    "\n",
    "noisy_importance.append(lda_feature_importance_100_random)\n",
    "noisy_importance.append(lda_feature_importance_300_random)\n",
    "noisy_importance.append(knn_feature_importance_100_random)\n",
    "noisy_importance.append(knn_feature_importance_300_random)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_multi(data_list, labels=None, xlabel='', ylabel='', title=''):\n",
    "\n",
    "    num_bars = len(data_list[0][0]['Feature importance'])\n",
    "    num_sets = len(data_list)\n",
    "\n",
    "    bar_width = 0.35\n",
    "    index = np.arange(num_bars)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for i in range(num_sets):\n",
    "        if labels:\n",
    "            plt.bar(index + i * bar_width, data_list[i], bar_width, label=labels[i])\n",
    "        else:\n",
    "            plt.bar(index + i * bar_width, data_list[i], bar_width)\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    if labels:\n",
    "        plt.legend()\n",
    "    plt.xticks(index + bar_width / 2, range(num_bars))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances_noise(model_data_list):\n",
    "    num_plots = len(model_data_list)\n",
    "    nrows = 2\n",
    "    ncols = 2\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(15, 10))\n",
    "    axs = axs.flatten()  # Flatten the 2D array of axes to easily index them in a loop\n",
    "    \n",
    "    for idx, model_data in enumerate(model_data_list):\n",
    "        if idx >= nrows * ncols:\n",
    "            break  # Avoid plotting more subplots than available slots\n",
    "\n",
    "        model_info = model_data[0][0]  # Take the first model info to extract model name\n",
    "        model_name = model_info['Model']\n",
    "        \n",
    "        features_list = []\n",
    "        importances_list = []\n",
    "        \n",
    "        for model_info in model_data[0]:\n",
    "            feature_importances = model_info['Feature importance']\n",
    "            sorted_importances = sorted(enumerate(feature_importances), key=lambda x: x[1], reverse=True)\n",
    "            features, importances = zip(*sorted_importances)\n",
    "            features_list.append(features)\n",
    "            importances_list.append(importances)\n",
    "        \n",
    "        ax = axs[idx]\n",
    "        y_pos = range(len(features_list[0]))  # Y positions for the bars\n",
    "        ax.bar(y_pos, importances_list[0], align='center')\n",
    "        ax.set_xticks([])  # Set x-ticks to be the indices\n",
    "        #ax.set_xticklabels([f'{i}' for i in features_list[0]])  # Label x-ticks with feature indices\n",
    "        ax.set_xlabel('Features')\n",
    "        ax.set_ylabel('Feature Importance')\n",
    "        ax.set_title(f'Feature Importances for {model_name}')\n",
    "        # ax.invert_yaxis()  # Invert y-axis to have the highest importance at the top\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for i in range(idx + 1, len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances_noise(noisy_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misslabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_missclasification(model, features, labels, iterations):\n",
    "    confusion_matrices = []\n",
    "    smote=SMOTE(random_state=42)\n",
    "    for iteration in tqdm(range(0, iterations)):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        X_train= scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        cm = confusion_matrix(y_pred=predictions, y_true=y_test)\n",
    "        confusion_matrices.append(cm)\n",
    "        \n",
    "    return confusion_matrices, model.classes_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_missclassification, knn_classes = evaluate_model_missclasification(knn_model,features,labels, 100)\n",
    "random_forest_missclassification, rf_classes = evaluate_model_missclasification(random_forest_model,features,labels, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_confusion_matrix(cm_list):\n",
    "    avg_cm = np.zeros_like(knn_missclassification[0])\n",
    "    for matrix in cm_list:\n",
    "        avg_cm += matrix\n",
    "    avg_cm = np.round(avg_cm/len(cm_list))\n",
    "    return  avg_cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_cm(cm, classes,model):\n",
    "\n",
    "    # Plot confusion matrix with labels using Seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f'Confusion Matrix {model}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_cm(average_confusion_matrix(knn_missclassification), knn_classes,'KNN')\n",
    "plot_cm(average_confusion_matrix(random_forest_missclassification),rf_classes,\"RandmoForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "classes = np.unique(labels)\n",
    "# Iterate over each class label\n",
    "for label in classes:  # Assuming labels are integers from 1 to 7\n",
    "    print(f\"Label {label}:\")\n",
    "    \n",
    "    # Iterate over each feature\n",
    "    for feature in unique_labels:  # Assuming you have 6 features\n",
    "        # Extract data for the specific label and feature\n",
    "        label_feature_data = features_df[labels_df == label][feature]\n",
    "        \n",
    "        # Perform the Shapiro-Wilk test for normality\n",
    "        statistic, p_value = stats.shapiro(label_feature_data)\n",
    "        \n",
    "        # Interpret the results\n",
    "        if p_value < 0.05:\n",
    "            print(f\"  Feature {feature}: Does not follow a normal distribution (reject null hypothesis)\")\n",
    "        else:\n",
    "            print(f\"  Feature {feature}: Follows a normal distribution (fail to reject null hypothesis)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
