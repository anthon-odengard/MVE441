{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/MyMNIST.csv', delimiter=',', index_col=False).dropna()\n",
    "# Split labels and features.\n",
    "print(data_df)\n",
    "labels_df = data_df[\"label\"]\n",
    "features_df = data_df.drop(columns=\"label\")\n",
    "\n",
    "labels = labels_df.to_numpy()\n",
    "features = features_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(labels, bins=10, edgecolor='k', alpha=0.7)\n",
    "plt.xticks(range(min(labels), max(labels) + 1))\n",
    "plt.xlabel(\"Number\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title('Class distribution of numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=42, test_size=0.9)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_model = PCA(n_components=500)\n",
    "principal_components = PCA_model.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Get eigenvalues\n",
    "eigenvalues = PCA_model.explained_variance_ratio_\n",
    "    \n",
    "    # Normalize eigenvalues\n",
    "\n",
    "\n",
    "    # Plot normalized eigenvalues\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(np.arange(1, len(eigenvalues) + 1), eigenvalues)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.title('Explained variance from PCA')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components_2D(ax, data, labels, pc1_index, pc2_index):\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange', 'purple', 'brown', 'pink']  # Define colors for different classes\n",
    "    \n",
    "    for target, color in zip(unique_labels, colors):\n",
    "        indices_to_keep = labels == target\n",
    "        ax.scatter(data[indices_to_keep, pc1_index], \n",
    "                   data[indices_to_keep, pc2_index],\n",
    "                   c=color,\n",
    "                   label=target,\n",
    "                   alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel(f'Principal Component {pc1_index+1}', fontsize=14)\n",
    "    ax.set_ylabel(f'Principal Component {pc2_index+1}', fontsize=14)\n",
    "    ax.set_title(f'PC{pc1_index+1} vs PC{pc2_index+1}', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Create the principal components data and labels (replace these with your actual data)\n",
    "# Assuming `principal_components` is a numpy array of shape (n_samples, n_components)\n",
    "# and `labels` is a numpy array of shape (n_samples,)\n",
    "# Example:\n",
    "# principal_components = np.random.rand(100, 4)  # Dummy data\n",
    "# labels = np.random.randint(0, 7, 100)  # Dummy labels (7 classes)\n",
    "\n",
    "# Create a 2x3 subplot grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each pair of principal components\n",
    "plot_principal_components_2D(axs[0, 0], principal_components, y_train, 0, 1)\n",
    "plot_principal_components_2D(axs[0, 1], principal_components, y_train, 0, 2)\n",
    "plot_principal_components_2D(axs[0, 2], principal_components, y_train, 0, 3)\n",
    "plot_principal_components_2D(axs[1, 0], principal_components, y_train, 1, 2)\n",
    "plot_principal_components_2D(axs[1, 1], principal_components, y_train, 1, 3)\n",
    "plot_principal_components_2D(axs[1, 2], principal_components, y_train, 2, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_kernel = KernelPCA(n_components=4, kernel='rbf', gamma=0.001)\n",
    "principal_components_kernel = PCA_kernel.fit_transform(X_train[:20000])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "plot_principal_components_2D(axs[0, 0], principal_components_kernel, y_train[:20000], 0, 1)\n",
    "plot_principal_components_2D(axs[0, 1], principal_components_kernel, y_train[:20000], 0, 2)\n",
    "plot_principal_components_2D(axs[0, 2], principal_components_kernel, y_train[:20000] ,0, 3)\n",
    "plot_principal_components_2D(axs[1, 0], principal_components_kernel, y_train[:20000] ,1, 2)\n",
    "plot_principal_components_2D(axs[1, 1], principal_components_kernel, y_train[:20000] ,1, 3)\n",
    "plot_principal_components_2D(axs[1, 2], principal_components_kernel, y_train[:20000] ,2, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 300  # Number of components you want to retain\n",
    "svd = TruncatedSVD(n_components=n_components)\n",
    "X_svd = svd.fit_transform(X_train)\n",
    "\n",
    "explained_variance_ratio = svd.explained_variance_ratio_\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.arange(1, n_components+1), explained_variance_ratio, 'o-', markersize=8, label='Explained Variance Ratio')\n",
    "plt.title('Explained Variance by SVD Components', fontsize=16)\n",
    "plt.xlabel('Component Number', fontsize=14)\n",
    "plt.ylabel('Explained Variance Ratio', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svd_2D(ax, data, labels, pc1_index, pc2_index):\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange', 'purple', 'brown', 'pink']  # Define colors for different classes\n",
    "    \n",
    "    for target, color in zip(unique_labels, colors):\n",
    "        indices_to_keep = labels == target\n",
    "        ax.scatter(data[indices_to_keep, pc1_index], \n",
    "                   data[indices_to_keep, pc2_index],\n",
    "                   c=color,\n",
    "                   label=target,\n",
    "                   alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel(f'Singular Vector {pc1_index+1}', fontsize=14)\n",
    "    ax.set_ylabel(f'Singular Vector {pc2_index+1}', fontsize=14)\n",
    "    ax.set_title(f'SVD (SV{pc1_index+1} vs SV{pc2_index+1})', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Assuming `X_svd` is a numpy array of shape (n_samples, n_components)\n",
    "# and `y_train` is a numpy array of shape (n_samples,)\n",
    "# Create a 2x3 subplot grid\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot each pair of singular vectors\n",
    "plot_svd_2D(axs[0, 0], X_svd, y_train, 0, 1)\n",
    "plot_svd_2D(axs[0, 1], X_svd, y_train, 0, 2)\n",
    "plot_svd_2D(axs[0, 2], X_svd, y_train, 0, 3)\n",
    "plot_svd_2D(axs[1, 0], X_svd, y_train, 1, 2)\n",
    "plot_svd_2D(axs[1, 1], X_svd, y_train, 1, 3)\n",
    "plot_svd_2D(axs[1, 2], X_svd, y_train, 2, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_tsne_2D(ax, data, labels, tsne1_index, tsne2_index):\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange', 'purple', 'brown', 'pink']  # Define colors for different classes\n",
    "    \n",
    "    for target, color in zip(unique_labels, colors):\n",
    "        indices_to_keep = labels == target\n",
    "        ax.scatter(data[indices_to_keep, tsne1_index], \n",
    "                   data[indices_to_keep, tsne2_index],\n",
    "                   c=color,\n",
    "                   label=target,\n",
    "                   alpha=0.3)\n",
    "    \n",
    "    ax.set_xlabel(f't-SNE Component {tsne1_index+1}', fontsize=14)\n",
    "    ax.set_ylabel(f't-SNE Component {tsne2_index+1}', fontsize=14)\n",
    "    ax.set_title(f't-SNE (Component {tsne1_index+1} vs Component {tsne2_index+1})', fontsize=16)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Assuming `X_tsne` is a numpy array of shape (n_samples, n_components)\n",
    "# and `y_train` is a numpy array of shape (n_samples,)\n",
    "# Create a 2x3 subplot grid\n",
    "fig, axs = plt.subplots(1,1, figsize=(18, 12))\n",
    "plot_tsne_2D(axs, X_tsne, y_train, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_values, p_values = f_classif(X_train, y_train)\n",
    "print(F_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = np.reshape(F_values, (28, 28))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(heatmap_data, cmap='turbo', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Heatmap of F-value')\n",
    "plt.xlabel('Column Index')\n",
    "plt.ylabel('Row Index')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'darkorange', 'purple', 'brown', 'pink']\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_label in np.unique(y_train):\n",
    "    plt.scatter(X_new[y_train == class_label, 0], X_new[y_train == class_label, 1],\n",
    "                color=colors[class_label], label=f'Class {class_label}', alpha=0.8)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Selected Features for Each Instance (Colored by Class Labels)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score,confusion_matrix, f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import BaggingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(features, labels, batch_size):\n",
    "    tensor_features = torch.tensor(features, dtype=torch.float32)\n",
    "    tensor_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    dataset = TensorDataset(tensor_features, tensor_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "def specificity_score(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    metrics= {}\n",
    "    for index, label in enumerate(labels):\n",
    "        TP = cm[index, index]\n",
    "        FP = cm[:,index].sum() - TP\n",
    "        FN = cm[:, index].sum() - TP\n",
    "        TN = cm.sum() - TP - FP - FN\n",
    "        \n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "        \n",
    "    specificity = np.mean(specificity)\n",
    "    \n",
    "    return specificity\n",
    "\n",
    "def eval_class_specific_performance(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    metrics= {}\n",
    "   \n",
    "    for index, label in enumerate(labels):\n",
    "        TP = cm[index, index]\n",
    "        FP = cm[:,index].sum() - TP\n",
    "        FN = cm[:, index].sum() - TP\n",
    "        TN = cm.sum() - TP - FP - FN\n",
    "        \n",
    "        accuracy = (TP + TN) /(TP+ FP+ FN+ TN)\n",
    "        recall = TP / (TP + FN) if (TP + FN) != 0 else 0\n",
    "        precision = TP / (TP + FP) if (TP + FP) != 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        \n",
    "        metrics[label] = {'accuracy': accuracy, \n",
    "                          'f1_score': f1_score}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_performance(model, features, labels, iterations):\n",
    "    evaluation_metrics = []\n",
    "    evaluation_metrics_class = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "    unique_labels= np.unique(labels)\n",
    "    \n",
    "    for iteration in tqdm(range(0, iterations)):\n",
    "        \n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        PCA_model = PCA(n_components=50)\n",
    "        X_train = PCA_model.fit_transform(X_train)\n",
    "        X_test = PCA_model.transform(X_test)\n",
    "        \n",
    "        bagging_classifier = BaggingClassifier(model, n_estimators=8, max_samples=0.2)\n",
    "        bagging_classifier.fit(X_train, y_train)\n",
    "        predictions = bagging_classifier.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions, average='macro')\n",
    "    \n",
    "    \n",
    "        \n",
    "        class_specific_metrics = eval_class_specific_performance(y_test, predictions, unique_labels)\n",
    "        \n",
    "        evaluation_metrics.append({\n",
    "            'overall_accuracy': accuracy,\n",
    "            'overall_f1': f1\n",
    "        })\n",
    "        evaluation_metrics_class.append({'class_specific_metrics': class_specific_metrics})\n",
    "    return evaluation_metrics, evaluation_metrics_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_model = LogisticRegression(C=1, penalty='l1', solver='liblinear', max_iter=2000)\n",
    "logistic_regression_perfomance, logistic_regression_perfomance_class = evaluate_model_performance(logistic_regression_model, features, labels, 10)\n",
    "print(logistic_regression_perfomance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_model_performance, knn_model_performance_class = evaluate_model_performance(knn_model, features, labels, 10)\n",
    "print(knn_model_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier(max_depth=20, n_jobs=-1)\n",
    "random_forest_performance, random_forest_performance_class = evaluate_model_performance(random_forest_model, features, labels, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LinearDiscriminantAnalysis(solver='svd')\n",
    "lda_performance, lda_performance_class = evaluate_model_performance(lda_model, features, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_model =GaussianNB(var_smoothing=1e-12)\n",
    "gaussian_performance, gaussian_performance_class = evaluate_model_performance(gaussian_model, features, labels, 10)\n",
    "print(gaussian_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfomance_data = [logistic_regression_perfomance, knn_model_performance, random_forest_performance, lda_performance, gaussian_performance]\n",
    "models = ['LogisticRegression', 'KNeighbors', 'RandomForest', 'LDA', 'NaiveBayes']\n",
    "# Function to create a DataFrame for a specific metric\n",
    "def create_metric_df(all_data, metric_index, metric_name, models):\n",
    "    df_list = []\n",
    "    for i, data in enumerate(all_data):\n",
    "        model_name = models[i]  # Get the model name from the models list\n",
    "        for metric_value in data:\n",
    "\n",
    "            df_list.append({'Model': model_name, metric_name: metric_value[metric_index]})\n",
    "    return pd.DataFrame(df_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = create_metric_df(perfomance_data, 'overall_accuracy', 'Accuracy', models)\n",
    "f1_score_df = create_metric_df(perfomance_data, 'overall_f1', 'f1_score', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create subplots with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Create boxplot for accuracy\n",
    "sns.boxplot(ax=axs[0], x='Model', y='Accuracy', data=accuracy_df)\n",
    "axs[0].set_title('Boxplot of Accuracy Across Models')\n",
    "axs[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Create boxplot for precision\n",
    "sns.boxplot(ax=axs[1], x='Model', y='f1_score', data=f1_score_df)\n",
    "axs[1].set_title('Boxplot of f1 score Across Models')\n",
    "axs[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metric_average(evaluation_metrics):\n",
    "    # Initialize dictionaries to store cumulative sums of metrics for each class\n",
    "    class_metrics_sum = {label: {'accuracy': 0, 'f1_score': 0} for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    class_counts = {label: 0 for label in evaluation_metrics[0]['class_specific_metrics']}\n",
    "    \n",
    "    # Calculate cumulative sums of metrics for each class\n",
    "    for metrics in evaluation_metrics:\n",
    "        for label, class_metrics in metrics['class_specific_metrics'].items():\n",
    "            class_metrics_sum[label]['accuracy'] += class_metrics['accuracy']\n",
    "            class_metrics_sum[label]['f1_score'] += class_metrics['f1_score']\n",
    "            class_counts[label] += 1\n",
    "    \n",
    "    # Calculate average metrics for each class\n",
    "    class_metrics_avg = {label: {metric: class_metrics_sum[label][metric] / class_counts[label] for metric in class_metrics_sum[label]} for label in class_metrics_sum}\n",
    "    return class_metrics_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_performance_data = [logistic_regression_perfomance_class, knn_model_performance_class, random_forest_performance_class, lda_performance_class, gaussian_performance_class]\n",
    "\n",
    "class_wise_performance = {}\n",
    "for model, model_class_metrics in zip(models, class_wise_performance_data):\n",
    "    class_wise_performance[model]= class_metric_average(model_class_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_data = {}\n",
    "f1_score_data = {}\n",
    "\n",
    "# Aggregate class-wise metrics for each model\n",
    "for model, class_metrics in class_wise_performance.items():\n",
    "    accuracy_data[model] = [metrics['accuracy'] for metrics in class_metrics.values()]\n",
    "    f1_score_data[model] = [metrics['f1_score'] for metrics in class_metrics.values()]\n",
    "\n",
    "# Convert aggregated metrics into a DataFrame\n",
    "accuracy_class_wise_df = pd.DataFrame(accuracy_data, index=class_metrics.keys())\n",
    "f1_class_wise_df = pd.DataFrame(f1_score_data, index=class_metrics.keys())\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "accuracy_class_wise_df.plot(kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_title('Accuracy by Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower left')  # Adjust legend position\n",
    "\n",
    "f1_class_wise_df.plot(kind='bar', ax=axes[1], rot=45)\n",
    "axes[1].set_title('Recall by Model')\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].legend(loc='lower left')  # Adjust legend position\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_missclasification(model, features, labels, iterations):\n",
    "    confusion_matrices = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\n",
    "    bagging_classifier = BaggingClassifier(model, n_estimators=8, max_samples=0.2)\n",
    "    for iteration in tqdm(range(0, iterations)):\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        PCA_model = PCA(n_components=50)\n",
    "        X_train = PCA_model.fit_transform(X_train)\n",
    "        X_test = PCA_model.transform(X_test)\n",
    "        \n",
    "        \n",
    "        bagging_classifier.fit(X_train, y_train)\n",
    "        predictions = bagging_classifier.predict(X_test)\n",
    "        \n",
    "        cm = confusion_matrix(y_pred=predictions, y_true=y_test)\n",
    "        confusion_matrices.append(cm)\n",
    "        \n",
    "    return confusion_matrices, bagging_classifier.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_missclassification, knn_classes = evaluate_model_missclasification(knn_model,features,labels, 5)\n",
    "random_forest_missclassification, rf_classes = evaluate_model_missclasification(random_forest_model,features,labels, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_confusion_matrix(cm_list):\n",
    "    avg_cm = np.zeros_like(knn_missclassification[0])\n",
    "    for matrix in cm_list:\n",
    "        avg_cm += matrix\n",
    "    avg_cm = np.round(avg_cm/len(cm_list))\n",
    "    return  avg_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(ax, cm, classes, model):\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes, ax=ax)\n",
    "    ax.set_title(f'Confusion Matrix {model}')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    ax.set_ylabel('True Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "classes =   range(0,10)\n",
    "\n",
    "plot_cm(axs[0], average_confusion_matrix(knn_missclassification), classes,'KNN')\n",
    "plot_cm(axs[1], average_confusion_matrix( random_forest_missclassification),rf_classes,\"RandmoForest\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_subset(features, labels, percentages):\n",
    "    # Initialize lists to store features and models\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Define the percentages to consider\n",
    "    # from 100% to 20% in 20% decrements\n",
    "    \n",
    "    for percentage in percentages:\n",
    "        # Split the data into train and test sets with varying test sizes (stratified)\n",
    "        print(percentage)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=percentage)\n",
    "        \n",
    "        # Create a model (you can replace this with any model creation logic)\n",
    "        # Example: Logistic Regression model\n",
    "        \n",
    "        # Append the selected features and the corresponding model to the lists\n",
    "        features_list.append(X_train)\n",
    "        labels_list.append(y_train)\n",
    "        \n",
    "    return zip(features_list, labels_list)\n",
    "\n",
    "def model_performance_reduced_data(model, subsets, percentages):\n",
    "    accuracy_list = []\n",
    "    f1_score_list = []\n",
    "\n",
    "    for features, labels in subsets:\n",
    "        print(features.shape)\n",
    "        # Call the function to evaluate model performance\n",
    "        model_performance, _ = evaluate_model_performance(model, features, labels, 5)\n",
    "        \n",
    "        # Extract the overall accuracy and F1 score from the performance metrics\n",
    "        accuracies = [metrics['overall_accuracy'] for metrics in model_performance]\n",
    "        f1_scores = [metrics['overall_f1'] for metrics in model_performance]\n",
    "        \n",
    "        # Calculate the average accuracy and F1 score for this iteration\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_f1_score = np.mean(f1_scores)\n",
    "        \n",
    "        # Append the average accuracy and F1 score to the respective lists\n",
    "        accuracy_list.append(avg_accuracy)\n",
    "        f1_score_list.append(avg_f1_score)\n",
    "        \n",
    "    return accuracy_list, f1_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = [0.99, 0.8, 0.6, 0.2, 0.1, 0.01]\n",
    "\n",
    "reduced_data = generate_subset(features,labels, percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = generate_subset(features,labels, percentages)\n",
    "logistic_regression_perfomance_reduced, f1_log = model_performance_reduced_data(logistic_regression_model, reduced_data, percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = generate_subset(features,labels, percentages)\n",
    "knn_perfomance_reduced, f1_knn = model_performance_reduced_data(knn_model, reduced_data, percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = generate_subset(features,labels, percentages)\n",
    "random_forest_perfomance_reduced, f1_rf = model_performance_reduced_data(random_forest_model, reduced_data, percentages)\n",
    "print(random_forest_perfomance_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = generate_subset(features,labels, percentages)\n",
    "lda_perfomance_reduced, f1_lda = model_performance_reduced_data(lda_model, reduced_data, percentages)\n",
    "print(lda_perfomance_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = generate_subset(features,labels, percentages)\n",
    "gaussian_perfomance_reduced, f1_gaussian = model_performance_reduced_data(gaussian_model, reduced_data, percentages)\n",
    "print(gaussian_perfomance_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_labels = ['LogisticRegression', 'KNN', 'RF', 'LDA', 'NaiveBayes']\n",
    "accuracy_scores = [logistic_regression_perfomance_reduced,knn_perfomance_reduced, random_forest_perfomance_reduced, lda_perfomance_reduced, gaussian_perfomance_reduced]\n",
    "f1_scores = [f1_log, f1_knn, f1_rf, f1_lda, f1_gaussian]\n",
    "print(accuracy_scores)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plotting accuracy scores\n",
    "for i in range(len(accuracy_scores)):\n",
    "    axs[0].plot(percentages, accuracy_scores[i], label=plt_labels[i] + ' Accuracy')\n",
    "\n",
    "axs[0].set_title('Accuracy Scores vs. Sample Size')\n",
    "axs[0].set_xlabel('Data Size')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "axs[0].invert_xaxis()  # Reverse x-axis\n",
    "\n",
    "# Plotting F1-scores\n",
    "for i in range(len(f1_scores)):\n",
    "    axs[1].plot(percentages, f1_scores[i], label=plt_labels[i] + ' F1-score')\n",
    "\n",
    "axs[1].set_title('F1-scores vs. Sample Size')\n",
    "axs[1].set_xlabel('Data Size')\n",
    "axs[1].set_ylabel('F1-score')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "axs[1].invert_xaxis()  # Reverse x-axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
